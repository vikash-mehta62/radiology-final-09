# Postmortem Template - Unified Reporting System

**Incident ID**: [INC-YYYY-NNNN]
**Date**: [YYYY-MM-DD]
**Author(s)**: [Names]
**Reviewers**: [Names]
**Status**: [Draft/Final]

---

## Executive Summary

[2-3 sentence summary of what happened, impact, and resolution]

**Example**: On January 15, 2024, the Unified Reporting System experienced a service disruption lasting 2 hours and 15 minutes, affecting approximately 40% of users. The root cause was database connection pool exhaustion due to a configuration error in the latest deployment. The issue was resolved by rolling back the deployment and increasing the connection pool size.

---

## Impact

### User Impact
- **Users Affected**: [NUMBER or PERCENTAGE]
- **Duration**: [START TIME] to [END TIME] ([DURATION])
- **Severity**: [P0/P1/P2]

### Business Impact
- **Reports Lost**: [NUMBER] (if any)
- **Reports Delayed**: [NUMBER]
- **Revenue Impact**: [IF APPLICABLE]
- **SLA Breach**: [YES/NO]

### Metrics
- **Error Rate**: Peak [X]%, Normal [Y]%
- **Autosave Success**: Dropped to [X]%
- **API Latency**: Peak [X]ms, Normal [Y]ms
- **Error Budget**: Consumed [X]% of monthly budget

---

## Timeline

All times in UTC. Include local time in parentheses if relevant.

| Time | Event | Action Taken |
|------|-------|--------------|
| 14:00 | Deployment started | v2.1.0 deployed to production |
| 14:15 | First alert fired | AutosaveSuccessRateLow alert |
| 14:17 | On-call acknowledged | Engineer began investigation |
| 14:25 | Root cause identified | Database connection pool exhausted |
| 14:30 | Mitigation started | Rollback initiated |
| 14:45 | Rollback completed | v2.0.9 restored |
| 15:00 | Service restored | Autosave success rate normalized |
| 15:15 | Monitoring continues | Verified stability |
| 16:15 | Incident closed | All metrics normal for 1 hour |

### Detailed Timeline

**14:00 UTC** - Deployment of v2.1.0 began
- Change: Updated database connection configuration
- Approver: [Name]
- Deployment method: Rolling update

**14:15 UTC** - Alert: AutosaveSuccessRateLow
- Autosave success rate dropped from 99.7% to 85%
- PagerDuty alert sent to on-call engineer
- Grafana dashboard showed spike in API errors

**14:17 UTC** - On-call engineer acknowledged
- Engineer [Name] acknowledged alert
- Began investigation in #incident-2024-001
- Checked recent deployments

**14:20 UTC** - Initial investigation
- Reviewed API logs: "Connection pool exhausted" errors
- Checked database: Connection count at maximum (100/100)
- Identified correlation with v2.1.0 deployment

**14:25 UTC** - Root cause identified
- Configuration error in v2.1.0: `maxPoolSize: 10` (should be 100)
- Decision made to rollback rather than hotfix
- Notified stakeholders

**14:30 UTC** - Rollback initiated
- Executed: `kubectl rollout undo deployment/api-server`
- Monitored rollback progress
- Updated status page

**14:45 UTC** - Rollback completed
- All pods running v2.0.9
- Connection pool size restored to 100
- Autosave success rate recovering

**15:00 UTC** - Service restored
- Autosave success rate: 99.5%
- API error rate: <0.5%
- User reports of issues stopped

**15:15 UTC** - Monitoring continues
- Verified all metrics stable
- No new errors in logs
- Database connections normal (45/100)

**16:15 UTC** - Incident closed
- All metrics normal for 1 hour
- Postmortem scheduled
- Incident channel archived

---

## Root Cause

### What Happened

[Detailed technical explanation of the root cause]

**Example**: The v2.1.0 deployment included a configuration change to the database connection pool settings. A typo in the configuration file set `maxPoolSize: 10` instead of `maxPoolSize: 100`. Under normal load, the application requires approximately 50-80 concurrent database connections. With only 10 connections available, requests began queuing, leading to timeouts and autosave failures.

### Why It Happened

[Explanation of how the root cause was introduced]

**Example**: The configuration change was part of a larger refactoring to consolidate database settings. During code review, the typo was not caught because:
1. The configuration file was not included in the PR diff (generated file)
2. Staging environment has lower load and did not expose the issue
3. No automated validation of connection pool size

### Why It Wasn't Caught Earlier

[Explanation of why existing safeguards failed]

**Example**:
1. **Code Review**: Configuration file was auto-generated and not reviewed
2. **Staging Tests**: Staging environment has 10x lower load than production
3. **Canary Deployment**: Issue manifested immediately at 10% rollout but was attributed to "normal variance"
4. **Monitoring**: No alert for connection pool utilization

---

## Contributing Factors

1. **Configuration Management**
   - Auto-generated configuration files not version controlled
   - No validation of critical configuration values

2. **Testing**
   - Staging environment not representative of production load
   - No load testing before deployment

3. **Monitoring**
   - No alert for database connection pool utilization
   - Canary metrics not properly evaluated

4. **Process**
   - Deployment proceeded despite early warning signs
   - Rollback decision delayed by investigation time

---

## Detection

### How We Detected It
- **Method**: Automated alert (AutosaveSuccessRateLow)
- **Time to Detect**: 15 minutes after deployment
- **Alert Effectiveness**: Good - alert fired as expected

### What Worked Well
- PagerDuty alert reached on-call engineer immediately
- Grafana dashboard clearly showed the issue
- Correlation with deployment was obvious

### What Could Be Improved
- Earlier detection during canary phase
- Connection pool utilization monitoring
- Automated rollback on critical metric degradation

---

## Resolution

### What We Did
1. Identified root cause (connection pool exhaustion)
2. Made rollback decision
3. Executed rollback to v2.0.9
4. Verified service restoration
5. Monitored for stability

### Why This Approach
- **Rollback vs Hotfix**: Rollback was faster and lower risk
- **Communication**: Kept stakeholders informed every 30 minutes
- **Verification**: Waited 1 hour of stable metrics before closing

### Time to Resolution
- **Detection to Identification**: 10 minutes
- **Identification to Mitigation**: 5 minutes
- **Mitigation to Resolution**: 30 minutes
- **Total**: 45 minutes (plus 1 hour verification)

---

## Corrective Actions

### Immediate (Completed)
- [x] Rollback to v2.0.9
- [x] Fix configuration error in v2.1.1
- [x] Add connection pool utilization alert
- [x] Document incident

### Short-term (This Week)
- [ ] Add configuration validation to CI/CD
- [ ] Improve staging environment to match production load
- [ ] Add automated rollback on critical metric degradation
- [ ] Review all auto-generated configuration files

**Owner**: [Name]
**Due**: [Date]

### Long-term (This Quarter)
- [ ] Implement configuration management system (e.g., Consul)
- [ ] Add load testing to deployment pipeline
- [ ] Improve canary analysis automation
- [ ] Add connection pool auto-scaling

**Owner**: [Name]
**Due**: [Date]

---

## Lessons Learned

### What Went Well
1. **Alert System**: Fired quickly and accurately
2. **Runbooks**: Command center runbook was helpful
3. **Communication**: Stakeholders kept informed
4. **Rollback**: Executed smoothly and quickly
5. **Team Response**: On-call engineer responded immediately

### What Went Wrong
1. **Configuration Management**: Auto-generated files not validated
2. **Testing**: Staging didn't catch the issue
3. **Canary**: Early warning signs ignored
4. **Monitoring**: Missing connection pool alerts
5. **Process**: Deployment continued despite concerns

### What We'll Do Differently
1. **Validate all configuration changes** in CI/CD
2. **Match staging to production** load characteristics
3. **Automate canary analysis** with strict thresholds
4. **Add comprehensive monitoring** for all critical resources
5. **Improve deployment process** with better gates

---

## Action Items

| Action | Owner | Due Date | Status |
|--------|-------|----------|--------|
| Fix configuration error | [Name] | 2024-01-16 | âœ… Done |
| Add connection pool alert | [Name] | 2024-01-16 | âœ… Done |
| Add config validation to CI | [Name] | 2024-01-20 | ðŸ”„ In Progress |
| Improve staging environment | [Name] | 2024-01-25 | ðŸ“‹ Planned |
| Implement auto-rollback | [Name] | 2024-02-01 | ðŸ“‹ Planned |
| Review all configs | [Name] | 2024-01-22 | ðŸ“‹ Planned |
| Load testing in CI | [Name] | 2024-02-15 | ðŸ“‹ Planned |
| Config management system | [Name] | 2024-03-31 | ðŸ“‹ Planned |

---

## Appendix

### Related Incidents
- [INC-2023-045]: Similar connection pool issue (6 months ago)
- [INC-2024-002]: Configuration error (2 weeks later)

### References
- Deployment PR: [LINK]
- Incident Channel: #incident-2024-001
- Grafana Dashboard: [LINK]
- Alert History: [LINK]

### Metrics

**Error Budget Impact**:
- Monthly budget: 0.5% (216 minutes)
- Consumed: 135 minutes (62.5% of budget)
- Remaining: 81 minutes (37.5%)

**SLA Compliance**:
- Autosave SLO: 99.5% (target) â†’ 98.2% (actual) âŒ Missed
- API Availability: 99.9% (target) â†’ 99.1% (actual) âŒ Missed

---

## Sign-Off

**Reviewed By**:
- Engineering Manager: [Name] - [Date]
- SRE Lead: [Name] - [Date]
- Product Manager: [Name] - [Date]

**Approved By**:
- VP Engineering: [Name] - [Date]

**Published**: [Date]
**Distribution**: Engineering, Product, Leadership

---

**Last Updated**: [Date]
**Version**: 1.0
